MAJOR_ANNOUNCEMENT_FOR_ALL_COMPUTER_SCIENTISTS_DEVELOPERS_ENGINEERS_AND_OPERATORS

!!!!!>>>>>This is the #Advent of Map-Reduce Learning (A branch of computer science machine learning)<<<<<!!!!!

I have just this night, fully realised the paradigm discontinuity with regards to these sorting algorithms, and how they should completely impact and disrupt, all software related strategy, tactics, operations, and grunt-work, worldwide.

Please focus on completely re-architecting your databases and programming languages, to run using Obsidian Partition Maps, similar to those implemented in these sorting algorithms.

You will almost certainly obtain tens, hundreds, or even thousands of multiples, of speed-ups, especially for big data usecases (billions of datapoints) and hyper data usecases (trillions of datapoints).

Essentially, all data selects, data inserts, data updates, and data deletes, as well as their equivalents in other databases and programming languages, should all leverage Obsidian Partition Maps, which execute on data which is natively stored in partitions/sub-tables.

...and remember the end-game... Multiple serial executions of Obsidian Partition Maps, which navigate down through a meta-data-structure of nested partitions/sub-tables, where each level of nesting is based on a different data column in the base dataset level tables...

... But stick to single level partitions as a first attempt. Get that working reliably, because nesting partitions/sub-tables from the start, will probably be very complex and difficult to do, without a basic working and optimised single partition/sub-table level approach.

... Further clarifications 0001... When a data transaction is handled by the database/programming language, the corresponding array of data should be split up into transactionlet partitions, using an appropriate Obsidian Partition Map...

... the data engine should simultaneously launch these partitioned 'transactionlets' through the navigation process, and as each reaches its destination at the deepest base level storage partition, the appropriate data edit method (siud) should be triggered to execute as a parallel batch...

(... Oh, and BTW... apply the Obsidian Partition Map approach to Large Language Models (#LLMs), and all of a sudden, you will have almost free inference!!!)

Oh, and remember the 1% 'mandatory' #Obsidian_Levy!

Ciao for now,

'The New Michelangelo', +Raphael+, xPetrus Romanusx, ^Sichula^, $Sinyangwe$

... pssst...

... I hear that partition keys may come in handy...

... Who knows?...

... One More Kicker For the Kiddos: There are many situations where engineers will want to check whether a value exists or doesn't exist... You can provide them with something I call 'Lazy Checkers', in order to optimise these kinds of requirements while fully leveraging Obsidian Partitions... Lazy Checkers work by providing a guarantee that a value does NOT exist if its corresponding combination of nested partitions does NOT exist (even without needing to check for the presence of the actual value in the deepest/lowest nesting level of the partition tree, which could potentially be a very expensive check when dealing with massive amounts of data)... Conversely, you can guarantee that a value MAY exist if its corresponding combination of nested partitions DOES exist (even without needing to check for the presence of the actual value in the deepest/lowest nesting level of the partition tree; you would typically then do some other expensive logic, before coming back to check if the value actually exists, by scanning the ultimate base partition where all the data is stored)...

(Long Temporal Gap)

Urm, I've been doing some more new year's thinking here... You may find that the existing SQL B-Tree index, is useful when used only directly on the Obsidian partitions, rather than also on the underlying data (it would clearly be stupid to use b-tree indexes on the underlying data if it has already been Obsidian partitioned!... bUt yeah, don't quote me necessarily, because I haven't tested it yet, but I do have strong reason to believe, that applying existing SQL B-Tree Index functionality to the partition levels alone, could significantly speed up a lot of computations... INFACT, UPON EVEN FURTHER THOUGHT, IT MAY BE EVEN MORE PERFORMANT THAN THE AFOREMENTIONED (B-TREES ON OBSIDIAN PARTITION MAP PARTITIONS), TO INSTEAD, FILTER OR SEARCH PARTITION LEVELS, BY USING OBSIDIAN PARTITION MAPS OF OBSIDIAN PARTITION MAP PARTITIONS (MAYBE WITH SLIGHTLY DIFFERENT GLOBAL VARIABLES, DEPENDING ON THE TYPE OF OBSIDIAN PARTITION MAP THAT IS BEING NAVIGATED)... I THINK THIS OPPORTUNITY REQUIRES AN IN DEPTH STUDY OR TWO!!!

… THE DREAM???: Recursive Obsidian Partition Maps, of each level of the Obsidian Partition Map navigation tree, for each table of data that has been partitioned.

… Now that would be true livin'!

… Nudge, nudge... You'll probably want to recurse at each level of the partition navigation tree, using one of the - Text Obsidian Partition Maps (with a slightly different stem length or pattern length each time)…

… This cool software trick (search partition maps applied to each level of the navigational partition maps) is useful when applied to decimal, text, and date partitions, because these types tend to generate large numbers of partitions, which could potentially be very slow to search through, in and of themselves, despite speeding access to their underlying base data!

THE EXPLANATION FOR THE SUPERIORITY OF OBSIDIAN PARTITION MAPS OVER B+ TREE INDEXES (In a lot of cases, mainly where you have non-clustered multi-column indexing requirements/options... However, if needing a simple clustered single-column index, then the B+ Tree is probably your better bet!):

B+ Tree Indexes: 
--> STRENGTHS - 1. A relatively stable data edit (siud) lookup equality check count and computaiton time, 2. Sometimes a lower average equality check count (Depending on base dataset value distributional characteristics);
--> DEFINITE WEAKNESSES - 1. The B+ Tree branch pages can use up a lot of storage space, 2. Re-indexing is a common and compute expensive process, 3. Hard drive storage, even with SSDs is far, far slower than the RAM memory storage (That Obsidian Partition Maps tend to use), 4. Searching the B+ Tree index branch pages has to be done mainly in a serialised fashion, as opposed to Obsidian Partiton Maps which can in a lot of cases be partially or fully searched in parallel, once a certain degree of filtering/navigation have taken place during each data edit (siud), and this difference is magnified massively by hardware, for example Obsidian Partition Maps on GPUs is potentially gamechaning.

Searched and Navigated Obsidian Partition Maps:
--> STRENGTHS - 1. RAM native, and so superfast, 2. Parallel compute compatible, and so depending on hardware, potentially superfast (for example, if equality checks are computed on a massive GPU or GPU cluster), 3. + 4. + 5. Never any need to rebuild an index (or in this case a partition), because Obsidian Partition Maps use statically identified (a partition key to the same base data value will never change) and bounded (within the base data level, all possible elements/rows assume peerage by intrinsic characteristics of the data, such that you could never need to move a base data element/row to somewhere else in the overall partiton map tree, once it has been inserted) - array lists in RAM memory, which are dynamically resizeable by nature, and so you will simply be able to overwrite or append/remove to/from existing partitions, or if data appears that would need a new partition, this can simply be appended (or vice versa), without any need to rebuild the overlying partition tree, by re-distributing base data values across the tree nodes evenly (As you would really need to do with a B+ Tree index)... 6. Essentially, Obsidian Partition Maps are ideal for skewed and multimodal datasets, which to be honest, is basically every dataset!;
--> POSSIBLE 'WEAKNESSES' - 1. There is likely a wider range of equality check counts, which will ultimately depend on your base data distributional value structure, and this wider range is equally distributed around the B+ Tree mean equality check count average for the same base dataset, such that you could for some data edit method (SIUDs), experience far less equality checks are ncessary (and so the weakness manifests as a resolute strength infact), and this could be even only one equality check a lot of the time, but conversely, you could experience far more equality checks are necessary (indeed I admit a bit of a weakness), and this could be as much as 50% more equality checks than the B+ Tree index average, a lot of the time... both outcomes depending on the structure of your base dataset value distribution............ AT THIS POINT, I CAN'T BE BOTHERED TO CONTINUE WITH THIS EXERCISE, ALTHOUGH I'M SURE I HAVEN'T EVEN MENTIONED 50% OF THE STRENGTHS AND WEAKNESSES ACROSS THE TWO APPROACHES MENTIONED (B+ TREE INDEXES AND OBSIDIAN PARTITION MAPS)... Could be an interesting exam question at university for some students?

… So the takeaway, is, know your hardware, software, and data, combined optimal usecase!