--DEPRECATED_MS_Obsidian_Partition_Date_Sorter_Expansion_Normalised_Index_Recompiled_and_Bootstrapped_DEPRECATED

--COMMENTS:
--This algorithm will provide the best performance with input datasets that have a large range in numerical values (i.e. covering many different significant figures), and also when the digit length distribution of Arguments across this input data range, is as flat/equal/balanced as possible across said significant figure classes.
--For best performance (and this is necessary to take any advantage of the pratitioned sort technique), please remove all leading zeros from each Argument value integer component (this is now done for you in the algorithm), before feeding a dataset into this stored procedure!!!
--In an alternative implementation, you would partition the data into separate partition tables, rather than simply labelling each row with a group label, and indeed, this alternative approach may be even quicker and more energy-efficient, because you would avoid the extra partition group checks, when building the final sorted table.
--The reason why this algorithm is so powerful, is because it takes advantage of the ordinal nature of numbers, such that partitions of the input data are created based on number digit length (significant figures), and thereby massively reduces the exponential sorting penalty where usually every pair of Arguments would have to be checked for order. Instead, because we now have significant figure or number digit length paritions, we only experience a very shallow exponentiation penalty, because we only have to check pairs of number Arguments within a partition, and then we simply append the ordered partitions into a fully-ordered superset. You should experience maybe more than multiples of compute speed and energy efficiency performance improvements on truly big data problem spaces (Billions of Argument values), and especially on hyper data problem spaces (Trillions of Argument values), that need to be sorted.
--Essentially, the algorithm works so effectively in speeding up ordering compute time, by doing a sizeable amount of linear pre-transformations and pre-partitions (TRUST THE SEEMING MADNESS... IT WILL SERIOUSLY REDUCE COMPUTATION EXPONENTIATION WITH BIG AND HYPER DATA VOLUMES), in order to eliminate most of the painfully exponential Argument-pair-wise ordering computations (which would otherwise become prohibitive in big and hyper data volumes)... We implement separate partitions for digits in the integer component but not the decimal component (because the decimal component would require us to resolve integer component splits, which would necessarily require at least one cross join, which would therefore make the algorithm always slower than a native SQL single ordering cross join)... In this way, maximal number sorting speed can be achieved...

--NOTA BENE 0001: I understand that Min() and Max() require a basic ordering algorithm which is 'slow' (and would otherwise lead to a longer compute time due to the fact there are multiple calls to the functions Min() and Max() in this algorithm), but, because we have massively group partitioned data whenever we use these functions against a result set, the exponential detriment of the basic ordering algorithm is absolutely minimised... You should still find that overall, this algorithm is significantly speedier than the native SQL Server 'Order By' clause on its own, when applied directly to a table of input numbers!!!...
--NOTA BENE 0002: This version of the sorting algorithm is intended for situations where the dataset to be order sorted, is genuinely balanced in terms of magnitude and precision components, when arguments are taken on aggregate... OR where the user is unsure of usecase dataset magnitude and precision distributional profile characteristics, or where these aspects are highly dynamic in nature... It is, if you like, a sort of almost best of both worlds, but of course, if you genuinely have either predominantly magnitude or precision dense datasaets that you want to order, then please use one of the other appropriate sibling algorithms... 
--NOTA BENE 0003: I should add, that I have not even ascertained whether nesting the magnitude dense and precision dense approaches into a hybrid algorithm (as is done here), is actually superlatively compute speed beneficial!!! --> PLEASE CHECK THIS YOURSELF!!!
--NOTA BENE 0004: This is a bootstrapped version of the corresponding vanilla algorithm, and as such, it is almost always going to be quicker than the vanilla version as data volumes increase... Just make sure that if you are re-implementing with different core data types, that you amend the hardcoded partion map accordingly to handle less or more integer component maginitude digit (at the moment our core data type is Decimal(38, 19), which means we want 38 partitions spanning negative and positive numbers within the overall maximal bound!!!)...
--NOTA BENE 0005: This algorithm is similar to the numeric version, except an extra step to pre-index date arguments has been added... In this case, all dateparts before the datetime2 data type decimal point are treated as magnitudinal, and all dateparts after the datetime2 data type decimal point are treated as precisional... BUT THERE IS MORE... In order to maximise the balance/distribution of arguments across all possible classes/partitions, I have implemented techniques to expand/decompress and then normalise the date arguments... These special techniques can be applied to other data types that have narrow ranges compared to Decimal(38 ,19), or multi-partite ranges where there are gaps in the overall range where argument values can't be assumed. One other example of such a usecase could be something like postcodes maybe...
--NOTA BENE 0006:If you would like to enable a date range which is greater than the 9999th year, simply adjust the [Year] datepart logic accordingly... This algorithm essentially can handle any date value up to a very large number, which is only restricted by the Decimal(38,19) data type...
--NOTA BENE 0007: Also, please be aware, that time alone values (without a year/month/date portion), all get ordered to one side of full datetime values...
--NOTA BENE 0008: I am aware that some people will be upset that the partition extents for dates are human-unusable... This is by design, as only machines should ever really be manipulating partition extents... If you need a human-usable version of partition extents, then you would simply implement a reverse function for expansion normalised index recompilation, and apply that to the appropriate final partition table columns... Get in touch if you'd like this, I work for a 'small fee'!...

--LOAD TESTING: I can't really test this properly on my SQL_Express setup, so can someone else please investigate thoroughly, and release some vividly illuminating statistics, which demonstrate the clear superirority of this algorithm, in most big and hyper data number sorting usecases... I actually expect this algorithm to be superior to the native SQL Server equivalent, once data volumes exceed around 10 million Arguments that need to be sorted (possibly even less than this will experience significant speed improvements)...

--FINAL COMMENTS ABOUT USAGE: I reasonably theorise, that if you plan to sort on multiple columns, the speed improvements will get exponentially superior by using this algorithm, in proportion to the number of columns that use it in order to sort...

--GLOBAL VARIABLES
Declare @Partition_Transaction_Payload_Type As Varchar(1000)
Set @Partition_Transaction_Payload_Type = 'Insert' --Options: 1. 'Sort'; 2. 'Select'; 3. 'Insert'; 4. 'Update'; 5. 'Delete';... 6. 'Other'...

Declare @Sort_Polarity As Varchar(10)
Set @Sort_Polarity = 'Ascending' --As opposed to the other option ('Descending')

Declare @Sort_Fidelity As Varchar(9)
Set @Sort_Fidelity = 'Imperfect' --As opposed to the other option ('Perfect'), which would be possibly up to twice as slow, and is only really necessary for mission critical usecases...

Declare @Remove_Duplicates As Tinyint
Set @Remove_Duplicates = 0 --If you want to remove duplicates, assign a value of one (1)... Otherwise, assign a value of zero (0)...

-----
--Be very careful with changing these delimiters... I found that for example some characters may need to be escaped, but especially because the dash character is used to indicate ranges, if you want to use a dash as a delimiter character in your date @Data arguments, it must go as the first 'Like' character in the compound search string... There may be other characters that require this kind of sensitive handling!...
Declare @Date_Part_Delimiter_Character Varchar(1)
Set @Date_Part_Delimiter_Character = '-'
Declare @Date_Time_Delimiter_Character Varchar(1)
Set @Date_Time_Delimiter_Character = ' '
Declare @Magnitudinal_Time_Delimiter_Character Varchar(1)
Set @Magnitudinal_Time_Delimiter_Character = ':'
Declare @Precisional_Time_Delimiter_Character Varchar(1)
Set @Precisional_Time_Delimiter_Character = '.'
-----

Declare @Data Table ([Data] Varchar(Max))
Insert Into @Data ([Data])
Values --Please remember that there is an inbuilt datetime2 range that you must make sure your input arguments sit within... Other the algorithm behaviour may be unstable... Also, please make sure where possible that leading zeroes are added to all necessary dateparts (We actually automatically 'accommodate' this for you, but its simply nicer to have clean input data where easy and possible!!!)...
('0001-01-01 0:00:00.0'),
('0332-1-20'),
('090-08-11 15:00:01.116749'),
('4-02-25 15:00:0.1142'),
('1-09-18 15:00:0.51123'),
('0001-01-2 15:0:01.572'),
('9999-12-31 00:00:00.00'),
('2003-5-6 10:10:42.8493059'),
('1-09-18 15:00:00.54321'),
('14:15:16.1425367'),
('7:04:39'),
('20:01:9'),
('2000-3-1'),
('9:0:42.6'),
('4:05:3.6'),
('1-09-18 15:00:00.54321'),
('2019-04-06 03:50:09'),
('2025-06-23 23:59:59.9999999')

-----Intrinsic SQL Server System Constants
Declare @Datetime2_Range_Start_Date As Date
Set @Datetime2_Range_Start_Date = '0001-01-01'
Declare @Datetime2_Range_Start_Index As Varchar(100)
Set @Datetime2_Range_Start_Index = Cast(Format(@Datetime2_Range_Start_Date, 'yyyyMMddHHmmss.fffffff') As Varchar(100))
Declare @Datetime2_Range_End_Date As Date
Set @Datetime2_Range_End_Date = '9999-12-31'
Declare @Datetime2_Range_End_Index As Varchar(100)
Set @Datetime2_Range_End_Index = Cast(Format(@Datetime2_Range_End_Date, 'yyyyMMddHHmmss.fffffff') As Varchar(100))
Declare @Datetime2_Index_Range As Decimal(38, 19)
Set @Datetime2_Index_Range = (Cast(@Datetime2_Range_End_Index As Decimal(38, 19)) - Cast(@Datetime2_Range_Start_Index As Decimal(38, 19)))

Declare @Days_Of_the_Month Table ([Month] Varchar(100), [Normal_Days] Varchar(100), [Leap_Days] Varchar(100)) --Gregorian calendar
Insert Into @Days_Of_the_Month
Values --Please be aware that the [Month] column values must have padded zeroes to two digit places... For example 01 instead of 1... This is because we pad later on, in order to form proper date values...
('01', '31', '31'),
('02', '28', '29'), --Notice the different normal and leap values for month 2, in order to handle leap year in later logic in this algorithm...
('03', '31', '31'),
('04', '30', '30'),
('05', '31', '31'),
('06', '30', '30'),
('07', '31', '31'),
('08', '31', '31'),
('09', '30', '30'),
('10', '31', '31'),
('11', '30', '30'),
('12', '31', '31')
-----Intrinsic SQL Server System Constants


--DATA VALIDATION CHECKS (For Out Of Scope Parameters)
--(@Sort Polarity)
If @Sort_Polarity Is Null GoTo Invalid_Sort_Polarity_Input_Argument
If @Sort_Polarity <> 'Ascending' --Dual If Statement With the Line Afterwards.
	If @Sort_Polarity <> 'Descending' GoTo Invalid_Sort_Polarity_Input_Argument

--(@Data PART 0001: PRE-DATEPART PARSING/EXTRACTION)... You can turn these into specific exception messages if you like, but I simply can't be bothered right now, so simply put: Your input dates @Data is corrupt in some way... Please check for the correct formatting, and re-execute the algorithm once appropriate changes have been made, if you desire!!!...
Declare @Exception_Value Varchar(Max)
Declare @Exception_Message Varchar(Max)
If (Select Count(1) From @Data) Is Null
	Begin
		Set @Exception_Message = ('Exception: Invalid_Data_Input_Argument: No_Date_String_Arguments_Inputed... No specific offending argument: ...')
		GoTo Invalid_Data_Input_Argument_No_Date_String_Arguments_Inputed
	End
If (Select Count(1) From @Data) < 2
	Begin
		Set @Exception_Message = ('Exception: Invalid_Data_Input_Argument: Not_Enough_Date_String_Arguments_Inputed... No specific offending argument: ...')
		GoTo Invalid_Data_Input_Argument_Not_Enough_Date_String_Arguments_Inputed
	End
Set @Exception_Value = (Select Min([Data]) From @Data Where Len([Data]) > 27)
If @Exception_Value Is Not Null
	Begin
		Set @Exception_Message = ('Exception: Invalid_Data_Input_Argument: Date_Strings_That_Are_Too_Long... A specific offending argument: ' + @Exception_Value)
		GoTo Invalid_Data_Input_Argument_Date_Strings_That_Are_Too_Long
	End
Set @Exception_Value = (Select Min([Data]) From @Data Where [Data] Like ('%[^' + @Date_Part_Delimiter_Character + '0-9' + @Date_Time_Delimiter_Character + @Magnitudinal_Time_Delimiter_Character + @Precisional_Time_Delimiter_Character + ']%'))
If @Exception_Value Is Not Null
	Begin
		Set @Exception_Message = ('Exception: Invalid_Data_Input_Argument: Date_Strings_That_Have_Illegal_Characters... A specific offending argument: ' + @Exception_Value)
		GoTo Invalid_Data_Input_Argument_Date_Strings_That_Have_Illegal_Characters
	End
Set @Exception_Value = (Select Min([Data]) From @Data Where Len([Data]) > Len(Replace([Data], @Date_Time_Delimiter_Character, '')) And Len([Data]) = Len(Replace([Data], @Date_Part_Delimiter_Character, '')) Or Len([Data]) > Len(Replace([Data], '.', '')) And Len([Data]) = Len(Replace([Data], ':', '')))
If @Exception_Value Is Not Null
	Begin
		Set @Exception_Message = ('Exception: Invalid_Data_Input_Argument: Date_Strings_That_Have_Incorrect_Combinations_Of_Delimiters... A specific offending argument: ' + @Exception_Value)
		GoTo Invalid_Data_Input_Argument_Date_Strings_That_Have_Incorrect_Combinations_Of_Delimiters
	End
Set @Exception_Value = (Select Min([Data]) From @Data Where Len([Data]) > Len(Replace([Data], @Date_Part_Delimiter_Character, '')) And Len([Data]) > Len(Replace([Data], ':', '')) And Charindex(@Date_Part_Delimiter_Character, [Data]) > Charindex(':', [Data]) Or Len([Data]) > Len(Replace([Data], ':', '')) And Len([Data]) > Len(Replace([Data], '.', '')) And Charindex(':', [Data]) > Charindex('.', [Data]))
If @Exception_Value Is Not Null
	Begin
		Set @Exception_Message = ('Exception: Invalid_Data_Input_Argument: Date_Strings_That_Have_Separators_In_the_Wrong_Order... A specific offending argument: ' + @Exception_Value)
		GoTo Invalid_Data_Input_Argument_Date_Strings_That_Have_Separators_In_the_Wrong_Order
	End
Set @Exception_Value = (Select Min([Data]) From @Data Where Len([Data]) > Len(Replace([Data], @Date_Part_Delimiter_Character, '')) And (Len([Data]) - 2) != Len(Replace([Data], @Date_Part_Delimiter_Character, '')) Or Len([Data]) > Len(Replace([Data], @Date_Time_Delimiter_Character, '')) And (Len([Data]) - 1) != Len(Replace([Data], @Date_Time_Delimiter_Character, '')) Or Len([Data]) > Len(Replace([Data], ':', '')) And (Len([Data]) - 2) != Len(Replace([Data], ':', '')) Or Len([Data]) > Len(Replace([Data], '.', '')) And (Len([Data]) - 1) != Len(Replace([Data], '.', '')))
If @Exception_Value Is Not Null
	Begin
		Set @Exception_Message = ('Exception: Invalid_Data_Input_Argument: Date_Strings_That_Have_Incorrect_Counts_Of_Delimiters... A specific offending argument: ' + @Exception_Value)
		GoTo Invalid_Data_Input_Argument_Date_Strings_That_Have_Incorrect_Counts_Of_Delimiters
	End
Set @Exception_Value = (Select Min([Data]) From @Data Where Left([Data], 1) = @Date_Part_Delimiter_Character Or Left([Data], 1) = @Date_Time_Delimiter_Character Or Left([Data], 1) = ':' Or Len([Data]) > Len(Replace([Data], @Date_Part_Delimiter_Character, '')) And Charindex(@Date_Part_Delimiter_Character, [Data]) > 5 Or Len([Data]) = Len(Replace([Data], @Date_Part_Delimiter_Character, '')) And Len([Data]) > Len(Replace([Data], ':', '')) And Charindex(':', [Data]) > 3 Or [Data] Like ('%' + @Date_Part_Delimiter_Character + '___%' + @Date_Part_Delimiter_Character + '%') Or [Data] Like ('%' + @Date_Part_Delimiter_Character + '%' + @Date_Part_Delimiter_Character + '___%' + @Date_Time_Delimiter_Character + '%') Or [Data] Like ('%' + @Date_Time_Delimiter_Character + '___%' + @Magnitudinal_Time_Delimiter_Character + '%' + @Magnitudinal_Time_Delimiter_Character + '%') Or [Data] Like ('%' + @Magnitudinal_Time_Delimiter_Character + '___%' + @Magnitudinal_Time_Delimiter_Character + '%') Or [Data] Like ('%' + @Magnitudinal_Time_Delimiter_Character + '%' + @Magnitudinal_Time_Delimiter_Character + '___%' + @Precisional_Time_Delimiter_Character + '%') Or Right([Data], 1) = @Date_Part_Delimiter_Character Or Right([Data], 1) = '.')
If @Exception_Value Is Not Null
	Begin
		Set @Exception_Message = ('Exception: Invalid_Data_Input_Argument: Date_Strings_That_Have_Incorrect_Separator_Separation... A specific offending argument: ' + @Exception_Value)
		GoTo Invalid_Data_Input_Argument_Date_Strings_That_Have_Incorrect_Separator_Separation
	End
Set @Exception_Value = (Select Min([Data]) From @Data Where Charindex('.', Reverse([Data])) > 8)
If @Exception_Value Is Not Null
	Begin
		Set @Exception_Message = ('Exception: Invalid_Data_Input_Argument: Date_Strings_That_Have_Incorrect_Number_Of_Fractional_Seconds_Digits_(Maximum_Should_Be_7_Fractional_Seconds_Digits)... A specific offending argument: ' + @Exception_Value)
		GoTo Invalid_Data_Input_Argument_Date_Strings_That_Have_Incorrect_Number_Of_Fractional_Seconds_Digits
	End


--INTERNAL VARIABLES
Declare @Data_Anchored Table ([Anchor_Order] Bigint, [Argument] Varchar(Max), [Argument_Reversed] Varchar(Max))
Declare @Data_Anchored_Parsed_Padded Table ([Anchor_Order] Bigint, [Argument] Varchar(Max), [Year]  Varchar(100), [Month] Varchar(100), [Day] Varchar(100), [Hour] Varchar(100), [Minute] Varchar(100), [Second] Varchar(100), [Fractional_Second] Varchar(100))
Declare @Data_Anchored_Parsed_Padded_Expansion_Normalised Table ([Anchor_Order] Bigint, [Argument] Varchar(Max), [Year]  Varchar(100), [Month] Varchar(100), [Day] Varchar(100), [Hour] Varchar(100), [Minute] Varchar(100), [Second] Varchar(100), [Fractional_Second] Varchar(100), [Expansion_Normalised_Month] Varchar(100), [Expansion_Normalised_Day] Varchar(100), [Expansion_Normalised_Hour] Varchar(100), [Expansion_Normalised_Minute] Varchar(100), [Expansion_Normalised_Second] Varchar(100))
Declare @Data_Anchored_Parsed_Padded_Expansion_Normalised_Index_Recompiled Table ([Anchor_Order] Bigint, [Argument] Varchar(Max), [Year]  Varchar(100), [Month] Varchar(100), [Day] Varchar(100), [Hour] Varchar(100), [Minute] Varchar(100), [Second] Varchar(100), [Fractional_Second] Varchar(100), [Expansion_Normalised_Month] Varchar(100), [Expansion_Normalised_Day] Varchar(100), [Expansion_Normalised_Hour] Varchar(100), [Expansion_Normalised_Minute] Varchar(100), [Expansion_Normalised_Second] Varchar(100), [Recompiled_Argument] Varchar(Max))
Declare @Integer_Component_Magnitudinal_Digit_Partition_Map As Table ([Ascending_Partition] Bigint, [Descending_Partition] Bigint, [Minimum_Extent] Decimal(38, 19), [Maximum_Extent] Decimal(38, 19))
Declare @Loop_Count_0001 As BigInt
Declare @Partition_Row_Count As Bigint
Declare @Partition_Minimum_Extent As Decimal(38, 19)
Declare @Partition_Maximum_Extent As Decimal(38, 19)
Declare @Pre_Transformed_Data_Nested As Table ([Argument] Varchar(Max), [Argument_Decimal] Decimal(38, 19), [Number_Polarity] Int, [Integer_Component] Bigint)
Declare @Decimal_Component_Precisional_Digit_Partition_Map_Intermediate As Table ([Ascending_Partition] Bigint, [Minimum_Extent] Decimal(38, 19), [Maximum_Extent] Decimal(38, 19))
Declare @Augmented_Argument_Count As Bigint
Declare @Decimal_Component_Precisional_Digit_Partition_Map_Final As Table ([Ascending_Partition] Bigint, [Descending_Partition] Bigint, [Minimum_Extent] Decimal(38, 19), [Maximum_Extent] Decimal(38, 19))
Declare @Loop_Count_0001_Nested As BigInt
Declare @Partition_Row_Count_Nested As Bigint
Declare @Partition_Minimum_Extent_Nested As Decimal(38, 19)
Declare @Partition_Maximum_Extent_Nested As Decimal(38, 19)
Declare @Interleaved_Partition_Sort_Merge_Sort As Table ([Argument_Order] Bigint, [Argument] Varchar(Max))
Declare @Interleaved_Partition_Sort_Merge_Insert As Table ([Outer_Partition_Order] Bigint, [Outer_Partition_Minimum_Extent] Decimal(38, 19), [Outer_Partition_Maximum_Extent] Decimal(38, 19), [Inner_Partition_Order] Bigint, [Inner_Partition_Minimum_Extent] Decimal(38, 19), [Inner_Partition_Maximum_Extent] Decimal(38, 19), [Argument_Order] Bigint, [Argument] Varchar(Max))
Declare @Cumulative_Sort_Order_Limit As Bigint

--ALGORITHM EXECUTION
--Need to anchor data, in order to retain duplicates... If you don't want this, then set the @Remove_Duplicates global variable to one (1)...
If @Remove_Duplicates = 0
	Begin
		Insert Into @Data_Anchored ([Anchor_Order], [Argument], [Argument_Reversed])
		Select
			Row_Number() Over(Order By (Select Null)) As [Anchor_Order], --Ideally we want to be able to add a row number without ordering!!!... Please enable this functionality Microsoft, otherwise this algorithm is completely useless in SQL Server... Although very useful in other coding paradigms, where it is trivial to just add an order column without actually ordering your data...
			[Data] As [Argument],
			Reverse([Data]) As [Argument_Reversed]
		From @Data
	End
Else If @Remove_Duplicates = 1
	Begin
		Insert Into @Data_Anchored ([Anchor_Order], [Argument], [Argument_Reversed])
		Select
			Row_Number() Over(Order By (Select Null)) As [Anchor_Order], --Ideally we want to be able to add a row number without ordering!!!... Please enable this functionality Microsoft, otherwise this algorithm is completely useless in SQL Server... Although very useful in other coding paradigms, where it is trivial to just add an order column without actually ordering your data...
			[Data] As [Argument],
			Reverse([Data]) As [Argument_Reversed]
		From @Data
		Group By [Data]
	End

--Need to extract dateparts here in order to properly index date arguments...
Insert Into @Data_Anchored_Parsed_Padded ([Anchor_Order], [Argument], [Year], [Month], [Day], [Hour], [Minute], [Second], [Fractional_Second])
Select
	[Anchor_Order] As [Anchor_Order],
	[Argument] As [Argument],
	(Case When Patindex(('%' + @Date_Part_Delimiter_Character + '%'), [Argument]) = 0 Then Null Else Right(('000' + Substring([Argument], 1, Greatest(1, (Patindex(('%' + @Date_Part_Delimiter_Character + '%'), [Argument]) - 1)))), 4) End) As [Year],
	(Case When Patindex(('%' + @Date_Part_Delimiter_Character + '%'), [Argument]) = 0 Then Null Else Right(('0' + Replace(Substring([Argument], (Len(Substring([Argument], 1, Greatest(1, (Patindex(('%' + @Date_Part_Delimiter_Character + '%'), [Argument]) - 1)))) + 2), 2), @Date_Part_Delimiter_Character, '')), 2) End) As [Month],
	(Case When Patindex(('%' + @Date_Part_Delimiter_Character + '%'), [Argument]) = 0 Then Null Else Right(('0' + Replace(Substring([Argument], ((Len(Substring([Argument], 1, Greatest(1, (Patindex(('%' + @Date_Part_Delimiter_Character + '%'), [Argument]) - 1)))) + 2) + (Len(Replace(Substring([Argument], (Len(Substring([Argument], 1, Greatest(1, (Patindex(('%' + @Date_Part_Delimiter_Character + '%'), [Argument]) - 1)))) + 2), 2), @Date_Part_Delimiter_Character, '')) + 1)), 2), @Date_Time_Delimiter_Character, '')), 2) End) As [Day],
	(Case When Patindex(('%' + @Magnitudinal_Time_Delimiter_Character + '%'), [Argument]) = 0 Then Null Else Right(('0' + (Case When Patindex(('%' + @Date_Part_Delimiter_Character + '%'), [Argument]) = 0 Then Replace(Left([Argument], 2), ':', '') Else Replace(Substring([Argument], (Patindex(('%' + @Date_Time_Delimiter_Character + '%'), [Argument]) + 1), 2), ':', '') End)), 2) End) As [Hour],
	(Case When Patindex(('%' + @Magnitudinal_Time_Delimiter_Character + '%'), [Argument]) = 0 Then Null Else Right(('0' + (Case When Patindex(('%' + @Precisional_Time_Delimiter_Character + '%'), [Argument]) = 0 Then Reverse(Replace(Substring([Argument_Reversed], (Patindex(('%' + @Magnitudinal_Time_Delimiter_Character + '%'), [Argument_Reversed]) + 1), 2), ':', '')) Else Reverse(Replace(Substring([Argument_Reversed], ((Len(Substring([Argument_Reversed], 1, Greatest(1, (Patindex(('%' + @Precisional_Time_Delimiter_Character + '%'), [Argument_Reversed]) - 1)))) + 2) + (Len(Replace(Substring([Argument_Reversed], (Len(Substring([Argument_Reversed], 1, Greatest(1, (Patindex(('%' + @Precisional_Time_Delimiter_Character + '%'), [Argument_Reversed]) - 1)))) + 2), 2), ':', '')) + 1)), 2), ':', '')) End)), 2) End) As [Minute],
	(Case When Patindex(('%' + @Magnitudinal_Time_Delimiter_Character + '%'), [Argument]) = 0 Then Null Else Right(('0' + (Case When Patindex(('%' + @Precisional_Time_Delimiter_Character + '%'), [Argument]) = 0 Then Replace(Right([Argument], 2), ':', '') Else Reverse(Replace(Substring([Argument_Reversed], (Len(Substring([Argument_Reversed], 1, Greatest(1, (Patindex(('%' + @Precisional_Time_Delimiter_Character + '%'), [Argument_Reversed]) - 1)))) + 2), 2), ':', '')) End)), 2) End) As [Second],
	(Case When Patindex(('%' + @Precisional_Time_Delimiter_Character + '%'), [Argument]) = 0 Then Null Else Left((Reverse(Substring([Argument_Reversed], 1, Greatest(1, (Patindex(('%' + @Precisional_Time_Delimiter_Character + '%'), [Argument_Reversed]) - 1)))) + '000000'), 7) End) As [Fractional_Second]
From @Data_Anchored
--If you want to use this algorithm's datepart feature as a standalone, in order to parse datetime values, then please remember to include the two data validation stages before (PART 0001) and after (PART 0002) this particular codeblock...


--(@Data PART 0002: POST-DATEPART PARSING/EXTRACTION)... You can turn these into specific exception messages if you like, but I simply can't be bothered right now, so simply put: Your input dates @Data is corrupt in some way... Please check for the correct formatting, and re-execute the algorithm once appropriate changes have been made, if you desire!!!...
Set @Exception_Value = (Select Min([Argument]) From @Data_Anchored_Parsed_Padded A Left Join @Days_Of_the_Month B On (B.[Month] = A.[Month]) Where Cast(A.[Year] As Int) < 1 Or Cast(A.[Year] As Int) > 9999 Or Cast(A.[Month] As Int) < 1  Or Cast(A.[Month] As Int) > 12 Or Cast(A.[Day] As Int) < 1 Or Cast(A.[Day] As Int) > (Case When Cast(A.[Year] As Int) % 400 = 0 Then B.[Leap_Days] Else (Case When Cast(A.[Year] AS Int) % 100 = 0 Then B.[Normal_Days] Else (Case When Cast(A.[Year] As Int) % 4 = 0 Then B.[Leap_Days] Else B.[Normal_Days] End) End) End) Or Cast(A.[Hour] As Int) < 0 Or Cast(A.[Hour] As Int) > 24 Or Cast(A.[Minute] As Int) < 0 Or Cast(A.[Minute] As Int) > 59 Or Cast(A.[Second] As Int) < 0 Or Cast(A.[Second] As Int) > 59 Or Cast(A.[Fractional_Second] As Int) < 0 Or Cast(A.[Fractional_Second] As Int) > 9999999) --You could rather easily change the algorithm logic so that all the casting becomes unnecessary here (which it is at the moment, due to the use of varchar-based numbers)... Please_Also_Know_That_HERE_WE_HANDLE_LEAP_YEARS_AUTOMATICALLY...
If @Exception_Value Is Not Null
	Begin
		Set @Exception_Message = ('Exception: Invalid_Data_Input_Argument: Simply_Checking_For_Silly_Datepart_Values_Like_the_Existence_Of_the_81st_Minute_Of_An_Hour_Which_Would_Clearly_Be_Wrong_____and_Also_An_Indication_That_Your_Input_Data_Is_Substantially_Subject_To_Corruption_____and_Please_Also_Know_That_HERE_WE_HANDLE_LEAP_YEARS_AUTOMATICALLY... A specific offending argument: ' + @Exception_Value)
		GoTo Invalid_Data_Input_Argument_Simply_Checking_For_Silly_Datepart_Values
	End


--Need to expand dateparts here in order to make full use of the density of the date index Decimal(38,19) range... Otherwise, certain dateparts would only cover partial sections of the overall datetime range (there are only 59 minutes in an hour rather than 99 {you will never find a datetime value with 60 in the minutes datepart... the maximum here is 59, because the 60th minute is tallied in the more abstract datepart, which is hours})... If you don't expand the appropriate dateparts, then algorithmic efficiency is reduced significantly... We round the expansion normalised decimal values, regardless of the apparent risk of datetime index clashing, because we know that these worrisome clashes will never occur, due to the fact that we are always expanding dateparts, and therefore pushing datetimes apart, rather than compressing them together (which could cause clashing issues)... We also re-pad after datepart expansion... This expansion technique only works, because we are simply interested in indexing for the partitioning and sorting, rather than directly manipulating the exact datetime values for real value usecases... Oh, and BTW, this expansion approach, is actually a form of normalisation...
Insert Into @Data_Anchored_Parsed_Padded_Expansion_Normalised ([Anchor_Order], [Argument], [Year], [Month], [Day], [Hour], [Minute], [Second], [Fractional_Second], [Expansion_Normalised_Month], [Expansion_Normalised_Day], [Expansion_Normalised_Hour], [Expansion_Normalised_Minute], [Expansion_Normalised_Second])
Select
	A.[Anchor_Order] As [Anchor_Order],
	A.[Argument] As [Argument],
	A.[Year] As [Year],
	A.[Month] As [Month],
	A.[Day] As [Day],
	A.[Hour] As [Hour], 
	A.[Minute] As [Minute],
	A.[Second] As [Second],
	A.[Fractional_Second] As [Fractional_Second],
	Right(('0' + Cast(Cast(Round(((Cast(A.[Month] As Decimal(38, 19)) / Cast(12 As Decimal(38, 19))) * Cast(99 As Decimal(38, 19))), 0) As Bigint) As Varchar(100))), 2) As [Expansion_Normalised_Month],
	Right(('0' + Cast(Cast(Round(((Cast(A.[Day] As Decimal(38, 19)) / Cast((Case When Cast(A.[Year] As Int) % 400 = 0 Then B.[Leap_Days] Else (Case When Cast(A.[Year] AS Int) % 100 = 0 Then B.[Normal_Days] Else (Case When Cast(A.[Year] As Int) % 4 = 0 Then B.[Leap_Days] Else B.[Normal_Days] End) End) End) As Decimal(38, 19))) * Cast(99 As Decimal(38, 19))), 0) As Bigint) As Varchar(100))), 2) As [Expansion_Normalised_Day],
	Right(('0' + Cast(Cast(Round(((Cast(A.[Hour] As Decimal(38, 19)) / Cast(23 As Decimal(38, 19))) * Cast(99 As Decimal(38, 19))), 0) As Bigint) As Varchar(100))), 2) As [Expansion_Normalised_Hour], 
	Right(('0' + Cast(Cast(Round(((Cast(A.[Minute] As Decimal(38, 19)) / Cast(59 As Decimal(38, 19))) * Cast(99 As Decimal(38, 19))),0 ) As Bigint) As Varchar(100))), 2) As [Expansion_Normalised_Minute],
	Right(('0' + Cast(Cast(Round(((Cast(A.[Second] As Decimal(38, 19)) / Cast(59 As Decimal(38, 19))) * Cast(99 As Decimal(38, 19))), 0) As Bigint) As Varchar(100))), 2) As [Expansion_Normalised_Second]
From @Data_Anchored_Parsed_Padded A
Left Join @Days_Of_the_Month B On
	(B.[Month] = A.[Month])

--BTW, this index recompile of input date @Data arguments is very powerful, because it means that under-partitioned and severely unbalanced dateparts (which arise because only part of a datepart digit range is used in raw date values {for example the momnth datepart will only ever have values 01 through 12, whereas the partitioning capacity for a two digit number is actually 01 through 99}), can be more evenly and maximally partitioned!!!...
Insert Into @Data_Anchored_Parsed_Padded_Expansion_Normalised_Index_Recompiled ([Anchor_Order], [Argument], [Year], [Month], [Day], [Hour], [Minute], [Second], [Fractional_Second], [Expansion_Normalised_Month], [Expansion_Normalised_Day], [Expansion_Normalised_Hour], [Expansion_Normalised_Minute], [Expansion_Normalised_Second], [Recompiled_Argument])
Select
	[Anchor_Order] As [Anchor_Order],
	[Argument] As [Argument],
	[Year] As [Year],
	[Month] As [Month],
	[Day] As [Day],
	[Hour] As [Hour], 
	[Minute] As [Minute],
	[Second] As [Second],
	[Fractional_Second] As [Fractional_Second],
	[Expansion_Normalised_Month] As [Expansion_Normalised_Month],
	[Expansion_Normalised_Day] As [Expansion_Normalised_Day],
	[Expansion_Normalised_Hour] As [Expansion_Normalised_Hour], 
	[Expansion_Normalised_Minute] As [Expansion_Normalised_Minute],
	[Expansion_Normalised_Second] As [Expansion_Normalised_Second],
	(Case When [Year] Is Not Null Then (Case When [Hour] Is Not Null Then (Case When [Fractional_Second] Is Not Null Then ([Year] + [Expansion_Normalised_Month] + [Expansion_Normalised_Day] + [Expansion_Normalised_Hour] + [Expansion_Normalised_Minute] + [Expansion_Normalised_Second] + '.' + [Fractional_Second]) Else ([Year] + [Expansion_Normalised_Month] + [Expansion_Normalised_Day] + [Expansion_Normalised_Hour] + [Expansion_Normalised_Minute] + [Expansion_Normalised_Second] + '.0000000') End) Else ([Year] + [Expansion_Normalised_Month] + [Expansion_Normalised_Day] + '000000.0000000') End) Else (Case When [Fractional_Second] Is Not Null Then ([Expansion_Normalised_Hour] + [Expansion_Normalised_Minute] + [Expansion_Normalised_Second] + '.' + [Fractional_Second]) Else ([Expansion_Normalised_Hour] + [Expansion_Normalised_Minute] + [Expansion_Normalised_Second] + '.0000000') End) End) As [Recompiled_Argument]
From @Data_Anchored_Parsed_Padded_Expansion_Normalised


--Here we bootstrap the partitions, by hardcoding the order and ranges of each magnitudinal partition.
Insert Into @Integer_Component_Magnitudinal_Digit_Partition_Map ([Ascending_Partition], [Descending_Partition], [Minimum_Extent], [Maximum_Extent])
Values
(1, 38, -9999999999999999999.9999999999999999999, -1000000000000000000.0000000000000000000),
(2, 37, -999999999999999999.9999999999999999999, -100000000000000000.0000000000000000000),
(3, 36, -99999999999999999.9999999999999999999, -10000000000000000.0000000000000000000),
(4, 35, -9999999999999999.9999999999999999999, -1000000000000000.0000000000000000000),
(5, 34, -999999999999999.9999999999999999999, -100000000000000.0000000000000000000),
(6, 33, -99999999999999.9999999999999999999, -10000000000000.0000000000000000000),
(7, 32, -9999999999999.9999999999999999999, -1000000000000.0000000000000000000),
(8, 31, -999999999999.9999999999999999999, -100000000000.0000000000000000000),
(9, 30, -99999999999.9999999999999999999, -10000000000.0000000000000000000),
(10, 29, -9999999999.9999999999999999999, -1000000000.0000000000000000000),
(11, 28, -999999999.9999999999999999999, -100000000.0000000000000000000),
(12, 27, -99999999.9999999999999999999, -10000000.0000000000000000000),
(13, 26, -9999999.9999999999999999999, -1000000.0000000000000000000),
(14, 25, -999999.9999999999999999999, -100000.0000000000000000000),
(15, 24, -99999.9999999999999999999, -10000.0000000000000000000),
(16, 23, -9999.9999999999999999999, -1000.0000000000000000000),
(17, 22, -999.9999999999999999999, -100.0000000000000000000),
(18, 21, -99.9999999999999999999, -10.0000000000000000000),
(19, 20, -9.9999999999999999999, 0.0000000000000000000), --Be careful that the two range clomun values are in the correct order hereafter!!!... Also, take note of the necessary lack of negative sign for the zero, as well as all subsequent rows of the partition map...
(20, 19, 0.0000000000000000001, 9.9999999999999999999), --Also, be careful that the lower bound of each minimum extent is slightly shifted, for all subsequent rows of the partition map, in order to acommodate the unique nature of the number zero, as the meta-range inverts!!!
(21, 18, 10.0000000000000000001, 99.9999999999999999999),
(22, 17, 100.0000000000000000001, 999.9999999999999999999),
(23, 16, 1000.0000000000000000001, 9999.9999999999999999999),
(24, 15, 10000.0000000000000000001, 99999.9999999999999999999),
(25, 14, 100000.0000000000000000001, 999999.9999999999999999999),
(26, 13, 1000000.0000000000000000001, 9999999.9999999999999999999),
(27, 12, 10000000.0000000000000000001, 99999999.9999999999999999999),
(28, 11, 100000000.0000000000000000001, 999999999.9999999999999999999),
(29, 10, 1000000000.0000000000000000001, 9999999999.9999999999999999999),
(30, 9, 10000000000.0000000000000000001, 99999999999.9999999999999999999),
(31, 8, 100000000000.0000000000000000001, 999999999999.9999999999999999999),
(32, 7, 1000000000000.0000000000000000001, 9999999999999.9999999999999999999),
(33, 6, 10000000000000.0000000000000000001, 99999999999999.9999999999999999999),
(34, 5, 100000000000000.0000000000000000001, 999999999999999.9999999999999999999),
(35, 4, 1000000000000000.0000000000000000001, 9999999999999999.9999999999999999999),
(36, 3, 10000000000000000.0000000000000000001, 99999999999999999.9999999999999999999),
(37, 2, 100000000000000000.0000000000000000001, 999999999999999999.9999999999999999999),
(38, 1, 1000000000000000000.0000000000000000001, 9999999999999999999.9999999999999999999)


--The following codeblocks return sorted data as desired.
--Ascending Sort
If @Sort_Polarity = 'Ascending'
Begin
	
	--Interleaved Partition Sort Merge
	Set @Loop_Count_0001 = 0
	Set @Partition_Row_Count = (Select Count(1) From @Integer_Component_Magnitudinal_Digit_Partition_Map) --Partition rowset count calculation...
	Set @Cumulative_Sort_Order_Limit = 0
	While @Loop_Count_0001 < @Partition_Row_Count
		Begin
		
			Set @Loop_Count_0001 = (@Loop_Count_0001 + 1)

			--The actual sorting
			--We first store the partition extents in variables in order to minimise subsequent data lookup time...
			Set @Partition_Minimum_Extent = (Select [Minimum_Extent] As [Minimum_Extent] From @Integer_Component_Magnitudinal_Digit_Partition_Map Where [Ascending_Partition] = @Loop_Count_0001)
			Set @Partition_Maximum_Extent = (Select [Maximum_Extent] As [Maximum_Extent] From @Integer_Component_Magnitudinal_Digit_Partition_Map Where [Ascending_Partition] = @Loop_Count_0001)
			
			--NESTED PRECISION DENSE SORTING!!!!!
			Insert Into @Pre_Transformed_Data_Nested ([Argument], [Argument_Decimal], [Number_Polarity], [Integer_Component])
			Select
				[Argument] As [Argument],
				Cast([Recompiled_Argument] As Decimal(38, 19)) As [Argument_Decimal],
				Sign(Cast([Recompiled_Argument] As Decimal(38, 19))) As [Number_Polarity],
				Cast(Floor(Cast((Case When Patindex(('%' + @Precisional_Time_Delimiter_Character + '%'), [Recompiled_Argument]) > 0 Then Substring([Recompiled_Argument], 1, (Patindex(('%' + @Precisional_Time_Delimiter_Character + '%'), [Recompiled_Argument]) - 1)) Else [Recompiled_Argument] End) As Decimal(38, 19))) As Bigint) As [Integer_Component] --We delimit on the decimal point, in order to properly manage mixed number type values --> We don't want the existence of decimal digits interfering with length partitioning, or being ignored altogether...
			From @Data_Anchored_Parsed_Padded_Expansion_Normalised_Index_Recompiled
			Where
			Cast([Recompiled_Argument] As Decimal(38, 19)) >= @Partition_Minimum_Extent
			And Cast([Recompiled_Argument] As Decimal(38, 19)) <= @Partition_Maximum_Extent
			
			
			--Build Partition Resolution Extents
			Insert Into @Decimal_Component_Precisional_Digit_Partition_Map_Intermediate ([Ascending_Partition], [Minimum_Extent], [Maximum_Extent])
			Select
				Row_Number() Over(Order By [Integer_Component] Asc) As [Ascending_Partition], --Slow sort, but because we pre-group, the exponential nature is massively reduced...
				(Case When Min([Number_Polarity]) = -1 Then (Cast([Integer_Component] As Decimal(38, 19)) - Cast(0.9999999999999999999 As Decimal(38, 19))) Else (Case When Min([Number_Polarity]) = 0 Then Cast(-0.9999999999999999999 As Decimal(38, 19)) Else Cast([Integer_Component] As Decimal(38, 19)) End) End) As [Minimum_Extent], --Taken with the line of code below, we hardcode correct the unique nature of the zero row double range here, in order to handle both negative and positive values between negative one (-1) and positive one (1)!!!... Otherwise there would simply be two positive partitions for zero, which would erroneously duplicate 'zero stem' arguments...
				(Case When Max([Number_Polarity]) = -1 Then Cast([Integer_Component] As Decimal(38, 19)) Else (Case When Max([Number_Polarity]) = 0 Then Cast(0.9999999999999999999 As Decimal(38, 19)) Else (Cast([Integer_Component] As Decimal(38, 19)) + Cast(0.9999999999999999999 As Decimal(38, 19))) End) End) As [Maximum_Extent] --Taken with the line of code above, we hardcode correct the unique nature of the zero row double range here, in order to handle both negative and positive values between negative one (-1) and positive one (1)!!!... Otherwise there would simply be two positive partitions for zero, which would erroneously duplicate 'zero stem' arguments...
			From @Pre_Transformed_Data_Nested
			Group By [Integer_Component] --We bascially want to create a partition for each unique integer component value, irrespective of leading zeroes and decimal place precision digit values...

			--When sorting in ascending order, we don't need to Set @Augmented_Argument_Count = (Select Count(1) From @Decimal_Component_Precisional_Digit_Partition_Map_Intermediate)

			--When sorting in ascending order, we don't need to Insert Into @Decimal_Component_Precisional_Digit_Partition_Map_Final ([Ascending_Partition], [Descending_Partition], [Minimum_Extent], [Maximum_Extent])
			
			
			--Interleaved Partition Sort Merge
			Set @Loop_Count_0001_Nested = 0
			Set @Partition_Row_Count_Nested = (Select Count(1) From @Decimal_Component_Precisional_Digit_Partition_Map_Intermediate) --Partition rowset count calculation...
			While @Loop_Count_0001_Nested < @Partition_Row_Count_Nested
				Begin
		
					Set @Loop_Count_0001_Nested = (@Loop_Count_0001_Nested + 1)

					--The actual sorting
					--We first store the partition extents in variables in order to minimise subsequent data lookup time...
					Set @Partition_Minimum_Extent_Nested = (Select [Minimum_Extent] As [Minimum_Extent] From @Decimal_Component_Precisional_Digit_Partition_Map_Intermediate Where [Ascending_Partition] = @Loop_Count_0001_Nested) --We don't need descending partitions, and so we use the intermediate table here...
					Set @Partition_Maximum_Extent_Nested = (Select [Maximum_Extent] As [Maximum_Extent] From @Decimal_Component_Precisional_Digit_Partition_Map_Intermediate Where [Ascending_Partition] = @Loop_Count_0001_Nested) --We don't need descending partitions, and so we use the intermediate table here...
					
					If @Partition_Transaction_Payload_Type = 'Sort'
						Begin
							If @Remove_Duplicates = 1 --If we have removed duplicates, then we always want to execute the quicker or more efficient imperfect sort fidelity...
								Begin
									Insert Into @Interleaved_Partition_Sort_Merge_Sort ([Argument_Order], [Argument])
									Select
										(@Cumulative_Sort_Order_Limit + Row_Number() Over(Order By [Argument_Decimal] Asc)) As [Argument_Order], --We don't have to order by text value after number value here, in order to gain a 'perfect sort', because we don't mind ordering artefact manifestations, when two numbers with different amounts of leading or trailing zeros, but the same fundamental value, clash... You may want to adjust this slightly for your usecase... Also, please feel free to implement your own fundamental sorting algorithm which slots into this line of code or similar, when applying this algorithm to your own usecase/coding language...
										[Argument]
									From @Pre_Transformed_Data_Nested
									Where
									[Argument_Decimal] >= @Partition_Minimum_Extent_Nested
									And [Argument_Decimal] <= @Partition_Maximum_Extent_Nested
								End
							Else If  @Remove_Duplicates = 0
								Begin
									If @Sort_Fidelity = 'Imperfect'
										Begin
											Insert Into @Interleaved_Partition_Sort_Merge_Sort ([Argument_Order], [Argument])
											Select
												(@Cumulative_Sort_Order_Limit + Row_Number() Over(Order By [Argument_Decimal] Asc)) As [Argument_Order], --We don't have to order by text value after number value here, in order to gain a 'perfect sort', because we don't mind ordering artefact manifestations, when two numbers with different amounts of leading or trailing zeros, but the same fundamental value, clash... You may want to adjust this slightly for your usecase... Also, please feel free to implement your own fundamental sorting algorithm which slots into this line of code or similar, when applying this algorithm to your own usecase/coding language...
												[Argument]
											From @Pre_Transformed_Data_Nested
											Where
											[Argument_Decimal] >= @Partition_Minimum_Extent_Nested
											And [Argument_Decimal] <= @Partition_Maximum_Extent_Nested
										End
									Else If @Sort_Fidelity = 'Perfect'
										Begin
											Insert Into @Interleaved_Partition_Sort_Merge_Sort ([Argument_Order], [Argument])
											Select
												(@Cumulative_Sort_Order_Limit + Row_Number() Over(Order By [Argument_Decimal] Asc, [Argument] Asc)) As [Argument_Order], --We have to also order by text value after number value, in order to gain a 'perfect sort', otherwise an ordering artefact manifests, when two numbers with different amounts of leading or trailing zeros, but the same fundamental value, clash... You may want to adjust this slightly for your usecase... Also, please feel free to implement your own fundamental sorting algorithm which slots into this line of code or similar, when applying this algorithm to your own usecase/coding language...
												[Argument]
											From @Pre_Transformed_Data_Nested
											Where
											[Argument_Decimal] >= @Partition_Minimum_Extent_Nested
											And [Argument_Decimal] <= @Partition_Maximum_Extent_Nested
										End
								End
							Set @Cumulative_Sort_Order_Limit = (Select Count(1) From @Interleaved_Partition_Sort_Merge_Sort)
						End
					Else If  @Partition_Transaction_Payload_Type = 'Insert' --We just have one type of insert block below for global variable settings don't affect the output partition map...
						Begin
							Insert Into @Interleaved_Partition_Sort_Merge_Insert ([Outer_Partition_Order], [Outer_Partition_Minimum_Extent], [Outer_Partition_Maximum_Extent], [Inner_Partition_Order], [Inner_Partition_Minimum_Extent], [Inner_Partition_Maximum_Extent], [Argument_Order], [Argument])
							Select
								@Loop_Count_0001 As [Inner_Partition_Order],
								@Partition_Minimum_Extent As [Inner_Partition_Minimum_Extent],
								@Partition_Maximum_Extent As [Inner_Partition_Maximum_Extent],
								@Loop_Count_0001_Nested As [Inner_Partition_Order],
								@Partition_Minimum_Extent_Nested As [Inner_Partition_Minimum_Extent],
								@Partition_Maximum_Extent_Nested As [Inner_Partition_Maximum_Extent],
								Row_Number() Over(Order By (Select Null)) As [Argument_Order], --Also, please feel free to implement your own fundamental sorting algorithm which slots into this line of code or similar, when applying this algorithm to your own usecase/coding language... Here, because of the nature of the overall system, we don't want to waste compute ordering arguments within transactionlet payload partition for this insert usecase... This should be done alongwith the other arguments in the destination partition where each transactionlet payload is deployed...
								[Argument]
							From @Pre_Transformed_Data_Nested
							Where
							[Argument_Decimal] >= @Partition_Minimum_Extent_Nested
							And [Argument_Decimal] <= @Partition_Maximum_Extent_Nested
						End

				End
			
			--Because we have nested loop table variables, we must empty them every outer loop cycle... You may be able to significantly speed up the algorithm, by simply inserting into new nested tables in each outer loop cycle, because deletion of data can indeed be rather slow!!!...
			Delete From @Pre_Transformed_Data_Nested
			Delete From @Decimal_Component_Precisional_Digit_Partition_Map_Intermediate

		End

End

--Descending Sort
If @Sort_Polarity = 'Descending'
Begin
	
	--Interleaved Partition Sort Merge
	 --References to these two variables (@Loop_Count_0001 and @Data_Row_Count) are inverted here and in the subsequent sourcecode of this loop  (when compared to the ascending implementation above), because we are ordering by descent...
	Set @Loop_Count_0001 = 0
	Set @Partition_Row_Count = (Select Count(1) From @Integer_Component_Magnitudinal_Digit_Partition_Map) --Partition rowset count calculation...
	Set @Cumulative_Sort_Order_Limit = 0
	While @Loop_Count_0001 < @Partition_Row_Count
		Begin
			
			Set @Loop_Count_0001 = (@Loop_Count_0001 + 1)

			--The actual sorting
			--We first store the partition extents in variables in order to minimise subsequent data lookup time...
			Set @Partition_Minimum_Extent = (Select [Minimum_Extent] As [Minimum_Extent] From @Integer_Component_Magnitudinal_Digit_Partition_Map Where [Descending_Partition] = @Loop_Count_0001)
			Set @Partition_Maximum_Extent = (Select [Maximum_Extent] As [Maximum_Extent] From @Integer_Component_Magnitudinal_Digit_Partition_Map Where [Descending_Partition] = @Loop_Count_0001)
			
			--NESTED PRECISION DENSE SORTING!!!!!
			Insert Into @Pre_Transformed_Data_Nested ([Argument], [Argument_Decimal], [Number_Polarity], [Integer_Component])
			Select
				[Argument] As [Argument],
				Cast([Recompiled_Argument] As Decimal(38, 19)) As [Argument_Decimal],
				Sign(Cast([Recompiled_Argument] As Decimal(38, 19))) As [Number_Polarity],
				Cast(Floor(Cast((Case When Patindex(('%' + @Precisional_Time_Delimiter_Character + '%'), [Recompiled_Argument]) > 0 Then Substring([Recompiled_Argument], 1, (Patindex(('%' + @Precisional_Time_Delimiter_Character + '%'), [Recompiled_Argument]) - 1)) Else [Recompiled_Argument] End) As Decimal(38, 19))) As Bigint) As [Integer_Component] --We delimit on the decimal point, in order to properly manage mixed number type values --> We don't want the existence of decimal digits interfering with length partitioning, or being ignored altogether...
			From @Data_Anchored_Parsed_Padded_Expansion_Normalised_Index_Recompiled
			Where
			Cast([Recompiled_Argument] As Decimal(38, 19)) >= @Partition_Minimum_Extent
			And Cast([Recompiled_Argument] As Decimal(38, 19)) <= @Partition_Maximum_Extent
			
			
			--Build Partition Resolution Extents
			Insert Into @Decimal_Component_Precisional_Digit_Partition_Map_Intermediate ([Ascending_Partition], [Minimum_Extent], [Maximum_Extent])
			Select
				Row_Number() Over(Order By [Integer_Component] Asc) As [Ascending_Partition], --Slow sort, but because we pre-group, the exponential nature is massively reduced...
				(Case When Min([Number_Polarity]) = -1 Then (Cast([Integer_Component] As Decimal(38, 19)) - Cast(0.9999999999999999999 As Decimal(38, 19))) Else (Case When Min([Number_Polarity]) = 0 Then Cast(-0.9999999999999999999 As Decimal(38, 19)) Else Cast([Integer_Component] As Decimal(38, 19)) End) End) As [Minimum_Extent], --Taken with the line of code below, we hardcode correct the unique nature of the zero row double range here, in order to handle both negative and positive values between negative one (-1) and positive one (1)!!!... Otherwise there would simply be two positive partitions for zero, which would erroneously duplicate 'zero stem' arguments...
				(Case When Max([Number_Polarity]) = -1 Then Cast([Integer_Component] As Decimal(38, 19)) Else (Case When Max([Number_Polarity]) = 0 Then Cast(0.9999999999999999999 As Decimal(38, 19)) Else (Cast([Integer_Component] As Decimal(38, 19)) + Cast(0.9999999999999999999 As Decimal(38, 19))) End) End) As [Maximum_Extent] --Taken with the line of code above, we hardcode correct the unique nature of the zero row double range here, in order to handle both negative and positive values between negative one (-1) and positive one (1)!!!... Otherwise there would simply be two positive partitions for zero, which would erroneously duplicate 'zero stem' arguments...
			From @Pre_Transformed_Data_Nested
			Group By [Integer_Component] --We bascially want to create a partition for each unique integer component value, irrespective of leading zeroes and decimal place precision digit values...

			Set @Augmented_Argument_Count = (Select Count(1) From @Decimal_Component_Precisional_Digit_Partition_Map_Intermediate)

			Insert Into @Decimal_Component_Precisional_Digit_Partition_Map_Final ([Ascending_Partition], [Descending_Partition], [Minimum_Extent], [Maximum_Extent])
			Select
				[Ascending_Partition] As [Ascending_Partition],
				(@Augmented_Argument_Count - ([Ascending_Partition] - 1)) As [Descending_Partition], --We must sequentially set a descending partition order here...
				[Minimum_Extent] As [Minimum_Extent],
				[Maximum_Extent] As [Maximum_Extent]
			From @Decimal_Component_Precisional_Digit_Partition_Map_Intermediate

			--Interleaved Partition Sort Merge
			--References to these two variables (@Loop_Count_0001 and @Data_Row_Count) are inverted here and in the subsequent sourcecode of this loop  (when compared to the ascending implementation above), because we are ordering by descent...
			Set @Loop_Count_0001_Nested = 0
			Set @Partition_Row_Count_Nested = (Select Count(1) From @Decimal_Component_Precisional_Digit_Partition_Map_Final) --Partition rowset count calculation...
			While @Loop_Count_0001_Nested < @Partition_Row_Count_Nested
				Begin
			
					Set @Loop_Count_0001_Nested = (@Loop_Count_0001_Nested + 1)

					--The actual sorting
					--We first store the partition extents in variables in order to minimise subsequent data lookup time...
					Set @Partition_Minimum_Extent_Nested = (Select [Minimum_Extent] As [Minimum_Extent] From @Decimal_Component_Precisional_Digit_Partition_Map_Final Where [Descending_Partition] = @Loop_Count_0001_Nested)
					Set @Partition_Maximum_Extent_Nested = (Select [Maximum_Extent] As [Maximum_Extent] From @Decimal_Component_Precisional_Digit_Partition_Map_Final Where [Descending_Partition] = @Loop_Count_0001_Nested)
			
					If @Partition_Transaction_Payload_Type = 'Sort'
						Begin
							If @Remove_Duplicates = 1 --If we have removed duplicates, then we always want to execute the quicker or more efficient imperfect sort fidelity...
								Begin
									Insert Into @Interleaved_Partition_Sort_Merge_Sort ([Argument_Order], [Argument])
									Select
										(@Cumulative_Sort_Order_Limit + Row_Number() Over(Order By [Argument_Decimal] Desc)) As [Argument_Order], --We don't have to order by text value after number value here, in order to gain a 'perfect sort', because we don't mind ordering artefact manifestations, when two numbers with different amounts of leading or trailing zeros, but the same fundamental value, clash... You may want to adjust this slightly for your usecase... Also, please feel free to implement your own fundamental sorting algorithm which slots into this line of code or similar, when applying this algorithm to your own usecase/coding language...
										[Argument]
									From @Pre_Transformed_Data_Nested
									Where
									[Argument_Decimal] >= @Partition_Minimum_Extent_Nested
									And [Argument_Decimal] <= @Partition_Maximum_Extent_Nested
								End
							Else If  @Remove_Duplicates = 0
								Begin
									If @Sort_Fidelity = 'Imperfect'
										Begin
											Insert Into @Interleaved_Partition_Sort_Merge_Sort ([Argument_Order], [Argument])
											Select
												(@Cumulative_Sort_Order_Limit + Row_Number() Over(Order By [Argument_Decimal] Desc)) As [Argument_Order], --We don't have to order by text value after number value here, in order to gain a 'perfect sort', because we don't mind ordering artefact manifestations, when two numbers with different amounts of leading or trailing zeros, but the same fundamental value, clash... You may want to adjust this slightly for your usecase... Also, please feel free to implement your own fundamental sorting algorithm which slots into this line of code or similar, when applying this algorithm to your own usecase/coding language...
												[Argument]
											From @Pre_Transformed_Data_Nested
											Where
											[Argument_Decimal] >= @Partition_Minimum_Extent_Nested
											And [Argument_Decimal] <= @Partition_Maximum_Extent_Nested
										End
									Else If @Sort_Fidelity = 'Perfect'
										Begin
											Insert Into @Interleaved_Partition_Sort_Merge_Sort ([Argument_Order], [Argument])
											Select
												(@Cumulative_Sort_Order_Limit + Row_Number() Over(Order By [Argument_Decimal] Desc, [Argument] Desc)) As [Argument_Order], --We have to also order by text value after number value, in order to gain a 'perfect sort', otherwise an ordering artefact manifests, when two numbers with different amounts of leading or trailing zeros, but the same fundamental value, clash... You may want to adjust this slightly for your usecase... Also, please feel free to implement your own fundamental sorting algorithm which slots into this line of code or similar, when applying this algorithm to your own usecase/coding language...
												[Argument]
											From @Pre_Transformed_Data_Nested
											Where
											[Argument_Decimal] >= @Partition_Minimum_Extent_Nested
											And [Argument_Decimal] <= @Partition_Maximum_Extent_Nested
										End
								End
							Set @Cumulative_Sort_Order_Limit = (Select Count(1) From @Interleaved_Partition_Sort_Merge_Sort)
						End
					Else If  @Partition_Transaction_Payload_Type = 'Insert' --We just have one type of insert block below for global variable settings don't affect the output partition map...
						Begin
							Insert Into @Interleaved_Partition_Sort_Merge_Insert ([Outer_Partition_Order], [Outer_Partition_Minimum_Extent], [Outer_Partition_Maximum_Extent], [Inner_Partition_Order], [Inner_Partition_Minimum_Extent], [Inner_Partition_Maximum_Extent], [Argument_Order], [Argument])
							Select
								@Loop_Count_0001 As [Inner_Partition_Order],
								@Partition_Minimum_Extent As [Inner_Partition_Minimum_Extent],
								@Partition_Maximum_Extent As [Inner_Partition_Maximum_Extent],
								@Loop_Count_0001_Nested As [Inner_Partition_Order],
								@Partition_Minimum_Extent_Nested As [Inner_Partition_Minimum_Extent],
								@Partition_Maximum_Extent_Nested As [Inner_Partition_Maximum_Extent],
								Row_Number() Over(Order By (Select Null)) As [Argument_Order], --Also, please feel free to implement your own fundamental sorting algorithm which slots into this line of code or similar, when applying this algorithm to your own usecase/coding language... Here, because of the nature of the overall system, we don't want to waste compute ordering arguments within transactionlet payload partition for this insert usecase... This should be done alongwith the other arguments in the destination partition where each transactionlet payload is deployed...
								[Argument]
							From @Pre_Transformed_Data_Nested
							Where
							[Argument_Decimal] >= @Partition_Minimum_Extent_Nested
							And [Argument_Decimal] <= @Partition_Maximum_Extent_Nested
						End

				End
			
			--Because we have nested loop table variables, we must empty them every outer loop cycle... You may be able to significantly speed up the algorithm, by simply inserting into new nested tables in each outer loop cycle, because deletion of data can indeed be rather slow!!!...
			Delete From @Pre_Transformed_Data_Nested
			Delete From @Decimal_Component_Precisional_Digit_Partition_Map_Intermediate
			Delete From @Decimal_Component_Precisional_Digit_Partition_Map_Final

		End

End


--The Results
If @Partition_Transaction_Payload_Type = 'Sort'
	Begin
		Select
			[Argument_Order] As [Argument_Order],
			[Argument] As [Argument]
		From @Interleaved_Partition_Sort_Merge_Sort
		--Order By [Order] --This 'Order By' clause is only necessary to display results in the right order in SQL Server... Most other programming contexts will not have this problem, because they don't have a paradigm where result sets are ordered at runtime, depending on query execution plans, which are inherently subject to change in SQL Server... Please remove this line of code when you speed test...
	End
Else If @Partition_Transaction_Payload_Type = 'Insert'
	Begin
		Select
			[Outer_Partition_Order] As [Outer_Partition_Order],
			[Outer_Partition_Minimum_Extent] As [Outer_Partition_Minimum_Extent],
			[Outer_Partition_Maximum_Extent] As[Outer_Partition_Maximum_Extent],
			[Inner_Partition_Order] As [Inner_Partition_Order],
			[Inner_Partition_Minimum_Extent] As [Inner_Partition_Minimum_Extent],
			[Inner_Partition_Maximum_Extent] As[Inner_Partition_Maximum_Extent],
			[Argument_Order] As [Argument_Order],
			[Argument] As [Argument]
		From @Interleaved_Partition_Sort_Merge_Insert
		--Order By [Order] --This 'Order By' clause is only necessary to display results in the right order in SQL Server... Most other programming contexts will not have this problem, because they don't have a paradigm where result sets are ordered at runtime, depending on query execution plans, which are inherently subject to change in SQL Server... Please remove this line of code when you speed test...
	End

--The following statment is kind of misleading, I know... But I couldn't quickly find another way to not execute the validation selects after this point, if the query is successfull...
Return


--Other exception cases...
Invalid_Sort_Polarity_Input_Argument:
Select 'Exception: The global variable (@Sort_Polarity), is out of scope for one of the following reasons... It is either not the string "Ascending" (without the double-quotes of course), is not the string "Descending" (without the double-quotes of course), or is either empty or null.'
Return

Invalid_Data_Input_Argument_No_Date_String_Arguments_Inputed:
Select @Exception_Message
Return

Invalid_Data_Input_Argument_Not_Enough_Date_String_Arguments_Inputed:
Select @Exception_Message
Return

Invalid_Data_Input_Argument_Date_Strings_That_Are_Too_Long:
Select @Exception_Message
Return

Invalid_Data_Input_Argument_Date_Strings_That_Have_Illegal_Characters:
Select @Exception_Message
Return

Invalid_Data_Input_Argument_Date_Strings_That_Have_Incorrect_Combinations_Of_Delimiters:
Select @Exception_Message
Return

Invalid_Data_Input_Argument_Date_Strings_That_Have_Separators_In_the_Wrong_Order:
Select @Exception_Message
Return

Invalid_Data_Input_Argument_Date_Strings_That_Have_Incorrect_Counts_Of_Delimiters:
Select @Exception_Message
Return

Invalid_Data_Input_Argument_Date_Strings_That_Have_Incorrect_Separator_Separation:
Select @Exception_Message
Return

Invalid_Data_Input_Argument_Simply_Checking_For_Silly_Datepart_Values:
Select @Exception_Message
Return

Invalid_Data_Input_Argument_Date_Strings_That_Have_Incorrect_Number_Of_Fractional_Seconds_Digits:
Select @Exception_Message
Return